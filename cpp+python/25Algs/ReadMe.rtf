{\rtf1\ansi\ansicpg1252\cocoartf1404\cocoasubrtf470
{\fonttbl\f0\fswiss\fcharset0 Helvetica;\f1\fnil\fcharset0 Menlo-Regular;}
{\colortbl;\red255\green255\blue255;\red34\green80\blue188;}
\margl1440\margr1440\vieww10800\viewh8400\viewkind0
\deftab720
\pard\pardeftab720\partightenfactor0

\f0\fs22 \cf0 \expnd0\expndtw0\kerning0
This code was used to generate the comparison of algorithms reported in the paper:\
\
J. W. Crandall et al., Cooperating with Machines, 
\i Nature Communications
\i0 , 2018.\
\
Note that detailed results are included in Supplementary Note 3 of that paper.  I\'92m happy for anyone to use this code if it can be of any use to them.  If you do use this code to assist you in any research, I would appreciate it if you would cite the above paper.\
\
Below is a little information to help you get started with the code, which is written in C++ for Mac.  I can\'92t vouch for how it will behavior on other systems.  I apologize that it isn\'92t perhaps in the most readable state (I have a habit of not commenting in my code, as I don\'92t think of other people perhaps using it).\
\
The basics are as follows:\
(1) The main code to pair two algorithms together in repeated games can be compiled using the makefile in the \'93++\'94 folder.\
\
(2) Once compiled, the code can be run as specified in the comment just before main in main.cpp.\'a0 For example, to run a 1000-round prisoners dilemma between S++ and MBRL-1 (br1), you would type:\
\
\pard\pardeftab720\partightenfactor0

\f1 \cf1 \cb2 ./game prisoners S++ br1 1000 1\
\pard\pardeftab720\partightenfactor0

\f0 \cf0 \cb1 \
The round-by-round results are printed out in the \'93log\'94 folder (actions taken by both players followed by the payoffs each received), while a summary of the results is printed out in the \'93results\'94 folder.\
\
(3) You can create new matrix games using the format of the other matrix games (you should be able to figure out by looking at the other games in the \'93games\'94 folder.  However, you should make sure that payoffs are normalized between 0 and 1, as I believe that I made this assumption when coding up some of the algorithms.\
\
(4) In our paper, we essentially compared 25 different algorithms.  The code provided includes more than 25 algorithms.\'a0 You can infer the algorithms we used in our comparison by looking at the \'93nameAgents()\'94 function in the file batch.cpp.\
\
(5) You can play around with different parameter settings for the algorithms.  The settings I used are reported in Supplementary Note 3 in the above cited paper.\
\
(6) The \'93games\'94 folder specifies the payoff matrices of the various games.  Again, if you decide to test the code with new games, please ensure that the payoffs of the game are normalized between 0 and 1, as the implementation assumes this to be the case for some algorithms.  If you try the algorithms out with payoff matrices that do not satisfy this constraint, I can\'92t vouch for the algorithms working correctly.\
\
(7) The \'93memory_one\'94 and \'93memory_two\'94 folders contain information to get the algorithms \'93memory1\'94 (Mem1) and \'93memory2\'94 (Mem2) to work correctly for each game we used in our comparison of algorithms.  Note that these two algorithms require the memory1 and memory2 strategies to be evolved.  I\'92ve already evolved (and saved) these strategies for many of the games, but if you try to run it on one that hasn\'92t been evolved, the system is set up to compute it automatically (which takes a few minutes) as long as you have compiled the code in the folder StochasticMemory using the make file first.  Note that if you change the payoff matrix in an existing game, without re-evolving the strategy, the new strategies won\'92t work.  Thus, if you change the payoff matrix of the game, you\'92 should delete the corresponding file in the Memory1 and Memory2 directories, respectively.\
\
If you really would like to use the code but are having a hard time getting it to run (or something seems strange when you do run it), please contact me at crandall@cs.byu.edu.  I\'92d be happy to help (though I can\'92t promise I\'92ll be able to respond right away).\
\
Jacob Crandall\
January 2018}